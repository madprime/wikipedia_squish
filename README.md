# wikipedia_squish

Analyzing Wikipedia traffic data and pulling down text versions of Wikipedia pages, and compressing
with a Huffman encoding for Joe Davis's _Malus ecclesia_ project.

## 2015 notes

### Figure out top pages from traffic

Get traffic stats...

    python download_random_counts.py random_pages
    mkdir random_pages_sorted
    mv random_pages/*/*_sorted.gz random_pages_sorted/

...did this until there were 120 randomly sampled hour traffic stats from
between Jan 1 2008 and Dec 31 2014.

Then to combine these traffic stats and get a median for each page...

    combine_en_random_counts.py random_pages_sorted

... which produced the file 'combined_random.gz'

Then sorted this on the median column:

    zcat combined_random.gz | sort --key=2nr,2 --key=1,1 | gzip -c > combined_random_sorted_median.gz

Convert the top pages to a page name per line (decoded):

    python get_top_page_names.py traffic/combined_random_sorted_median.gz top_100000_pages

### Get the page text

I used many rounds of commands like the following...

    head -1000 top_100000_pages | tail -400 > page_titles_601_1000
    mkdir pages_601_1000
    python get_page_text.py page_titles_601_1000 pages_601_1000 temp.zip

To get text versions of all pages. Note that these required the git submodules.

### Flatten text

After putting all the current text versions together in a directory, the
following script "flattens" them, which includes converting them to all
uppercase and removing spaces and tabs.

This step also checks the content of the pages and skips over pages that have
duplicate content. These represent pages that were redirects (and we already
have the main version).

    python encoding/flatten_articles.py top_100000_pages pages_1_7522 pages_1_7522_flattened_nospace_uppercase

### Figure out the encoding

The following script is full of cruft I'm no longer using, having decided to
not bother with finding optimal digraphs, trigraphs, etc. Just single chars.

Anyway, it was still usable for generating the Huffman encoding by running:

    python encoding/calc_freq_2.py pages_1_7522_flattened_nospace_uppercase

I took the resulting dictionary and manually converted to a canonical
Huffman code.

Note: The combined bit cost of the set of least popular punctuation is
less than 1% of the total bit cost. Eliminating these would not result
in notable compression improvement: [];/%{&+$_?}\=!*#^<>|@`~

I saved that file in "to_canonical_code.csv"

### Encode text

I ran more text retrieval and flattening to get up to the top 21,522 entries
in 'top_100000_pages'. Not all of these work out, and some turn out to be
redundant redirects, so the total number turns out to be 21,346.

Running the following script, which used the huffman code csv and applied it
to the flattened text:

    python encoding/convert_pages_to_binary.py top_100000_pages pages_1_21522_flattened_nospace_uppercase pages_1_21522_flattened_nospace_uppercase_binary

### Large files

Large files generated by the above processes can be found in this Google
Drive directory (.tar.bz2 format).

https://drive.google.com/drive/u/0/folders/0B09Wmb_OL6XjfmZ2M1RZUzlnN2VfdFFzUEgwZEl5NUljVVR6dDJfX0Y3ai15UHNBVnROamc
